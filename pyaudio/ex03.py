import pyaudio
import wave
import numpy as np
import webrtcvad
import struct
import time
from datetime import datetime
import os
from scipy.fftpack import fft

class ImprovedVoiceRecorder:
    def __init__(self, debug_mode=False):
        # ì˜¤ë””ì˜¤ ì„¤ì •
        self.FORMAT = pyaudio.paInt16
        self.CHANNELS = 1
        self.RATE = 16000  # WebRTC VADëŠ” 8000, 16000, 32000, 48000Hzë§Œ ì§€ì›
        self.CHUNK = 480  # VADì— ì í•©í•œ í”„ë ˆì„ í¬ê¸° (30ms)
        self.SILENCE_THRESHOLD = 3.0  # ë¬´ìŒ ê°ì§€ ì„ê³„ê°’ (ì´ˆ)
        self.VOICE_THRESHOLD = 300    # ìŒì„± ê°ì§€ ì„ê³„ê°’ (í™˜ê²½ì— ë”°ë¼ ì¡°ì •)
        
        # VAD ì„¤ì •
        self.vad = webrtcvad.Vad()
        self.vad.set_mode(0)  # ê°€ì¥ ì—„ê²©í•œ ëª¨ë“œë¡œ ì„¤ì • (0: ë‚®ìŒ ~ 3: ë†’ìŒ)
        
        # ìƒíƒœ ë³€ìˆ˜
        self.recording = False
        self.frames = []
        self.silence_frames = 0
        self.consecutive_speech = 0
        self.required_speech_frames = 3  # ì•½ 0.09ì´ˆ ë™ì•ˆ ì—°ì† ê°ì§€ í•„ìš” (ê°ì§€ ì‹œê°„ ë‹¨ì¶•)
        self.output_dir = "recordings"
        self.debug_mode = debug_mode
        
        # ì£¼íŒŒìˆ˜ ë¶„ì„ìš© ì„¤ì •
        self.human_freq_low = 85    # ì‚¬ëŒ ëª©ì†Œë¦¬ ì£¼íŒŒìˆ˜ ë²”ìœ„ (Hz)
        self.human_freq_high = 255
        
        # ìŠ¤ë¬´ë”©ì„ ìœ„í•œ ë³€ìˆ˜
        self.smoothed_rms = 0
        self.smoothing_factor = 0.3  # ë‚®ì„ìˆ˜ë¡ ë¶€ë“œëŸ¬ì›Œì§
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.output_dir, exist_ok=True)
        
        # PyAudio ì´ˆê¸°í™”
        self.p = pyaudio.PyAudio()
        
    def start(self):
        """ë…¹ìŒ ì‹œìŠ¤í…œ ì‹œì‘"""
        print("ê°œì„ ëœ ìŒì„± ê°ì§€ ì‹œìŠ¤í…œ ì‹œì‘... (ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”)")
        
        # ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¼ ì—´ê¸°
        self.stream = self.p.open(
            format=self.FORMAT,
            channels=self.CHANNELS,
            rate=self.RATE,
            input=True,
            frames_per_buffer=self.CHUNK
        )
        
        # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì‹¤í–‰
        self.calibrate()
        
        try:
            while True:
                # ì˜¤ë””ì˜¤ ë°ì´í„° ì½ê¸°
                audio_data = self.stream.read(self.CHUNK, exception_on_overflow=False)
                
                # ìŒì„± ê°ì§€ (ë³µí•©ì  ë°©ë²•)
                is_speech = self.detect_speech(audio_data)
                
                # ì—°ì†ì ì¸ ìŒì„± í”„ë ˆì„ ì¹´ìš´íŒ…
                if is_speech:
                    self.consecutive_speech += 1
                else:
                    self.consecutive_speech = 0
                
                # ì¼ì • ê¸°ê°„ ì´ìƒ ì—°ì†í•´ì„œ ìŒì„±ì´ ê°ì§€ë˜ë©´ ë…¹ìŒ ì‹œì‘
                if self.consecutive_speech >= self.required_speech_frames and not self.recording:
                    self.start_recording()
                
                if self.recording:
                    # ë°ì´í„° ì €ì¥
                    self.frames.append(audio_data)
                    
                    # ì¤‘ìš”: ë…¹ìŒ ì¤‘ì§€ ë¡œì§ ìˆ˜ì • - ìŒì„± ê°ì§€ ì¡°ê±´ê³¼ ë™ì¼í•œ ì¡°ê±´ ì‚¬ìš©
                    is_silent = not is_speech  # ìŒì„± ê°ì§€ì™€ ë™ì¼í•œ ë¡œì§ ì‚¬ìš©
                    
                    # ìŒì„± ê°ì§€ ì—¬ë¶€ì— ë”°ë¼ ë¬´ìŒ ì¹´ìš´í„° ê°±ì‹ 
                    if not is_silent:
                        self.silence_frames = 0
                    else:
                        self.silence_frames += 1
                    
                    # ë¬´ìŒ ì§€ì† ì‹œê°„ ê³„ì‚° (ì´ˆ)
                    silence_duration = (self.silence_frames * self.CHUNK) / self.RATE
                    
                    # ë””ë²„ê·¸ ì¶œë ¥ì— ë¬´ìŒ ì‹œê°„ í‘œì‹œ
                    if self.debug_mode and self.recording:
                        print(f" [ë¬´ìŒ: {silence_duration:.1f}/{self.SILENCE_THRESHOLD:.1f}ì´ˆ]", end="")
                    
                    # ì§€ì •ëœ ì‹œê°„ ì´ìƒ ë¬´ìŒì´ë©´ ë…¹ìŒ ì¢…ë£Œ
                    if silence_duration >= self.SILENCE_THRESHOLD:
                        self.stop_recording()
        
        except KeyboardInterrupt:
            print("\ní”„ë¡œê·¸ë¨ ì¢…ë£Œ")
            
        finally:
            self.cleanup()
    
    def calibrate(self):
        """ì£¼ë³€ í™˜ê²½ ì†ŒìŒì„ ì¸¡ì •í•˜ì—¬ ì„ê³„ê°’ ìë™ ì¡°ì • - í™•ì¥ ë²„ì „"""
        print("í™˜ê²½ ì†ŒìŒ ì¸¡ì • ì¤‘... (10ì´ˆ)")
        samples = []
        
        # 10ì´ˆ ë™ì•ˆ ìƒ˜í”Œ ìˆ˜ì§‘ (5ì´ˆì—ì„œ 10ì´ˆë¡œ ì¦ê°€)
        for _ in range(int(self.RATE / self.CHUNK * 10)):
            audio_data = self.stream.read(self.CHUNK, exception_on_overflow=False)
            rms = self.get_rms(audio_data)
            samples.append(rms)
        
        # ë°°ê²½ ì†ŒìŒ ë ˆë²¨ ê³„ì‚° (í•˜ìœ„ 80%ì˜ í‰ê·  ì‚¬ìš©)
        samples.sort()
        valid_samples = samples[:int(len(samples) * 0.8)]  # ìƒìœ„ 20% ì œì™¸ (ì¼ì‹œì  ì†ŒìŒ ì œê±°)
        background_noise = np.mean(valid_samples)
        noise_std = np.std(valid_samples)
        
        # ì„ê³„ê°’ ì„¤ì • (ë°°ê²½ ì†ŒìŒ + í‘œì¤€í¸ì°¨ì˜ 4ë°°)
        self.VOICE_THRESHOLD = background_noise + noise_std * 4.0
        
        print(f"ì¸¡ì • ì™„ë£Œ. ë°°ê²½ ì†ŒìŒ ë ˆë²¨: {background_noise:.2f}")
        print(f"ìŒì„± ê°ì§€ ì„ê³„ê°’: {self.VOICE_THRESHOLD:.2f}ë¡œ ì„¤ì •ë¨")
    
    def detect_speech(self, audio_data):
        """ê°œì„ ëœ ìŒì„± ê°ì§€ ë¡œì§"""
        # WebRTC VADë¡œ ìŒì„± ê°ì§€
        try:
            vad_result = self.vad.is_speech(audio_data, self.RATE)
        except:
            vad_result = False
        
        # RMS ê¸°ë°˜ ê°ì§€
        rms = self.get_rms(audio_data)
        self.smoothed_rms = self.smoothing_factor * rms + (1 - self.smoothing_factor) * self.smoothed_rms
        rms_result = self.smoothed_rms > self.VOICE_THRESHOLD
        
        # ì£¼íŒŒìˆ˜ ë¶„ì„ ê¸°ë°˜ ê°ì§€
        freq_result = self.check_human_freq(audio_data)
        
        # ë””ë²„ê·¸ ëª¨ë“œì—ì„œëŠ” ìƒì„¸ ì •ë³´ ì¶œë ¥
        if self.debug_mode:
            status = "ğŸ”´" if self.recording else "âšª"
            # ê° ì¡°ê±´ë³„ ì•„ì´ì½˜ í‘œì‹œ
            vad_icon = "âœ“" if vad_result else "âœ—"
            rms_icon = "âœ“" if rms_result else "âœ—"
            freq_icon = "âœ“" if freq_result else "âœ—"
            speech = "ğŸ—£ï¸" if ((vad_result and rms_result) or (rms_result and freq_result) or (vad_result and freq_result)) else "  "
            
            # ë””ë²„ê·¸ ì •ë³´ ìƒì„¸ ì¶œë ¥
            print(f"{status} VAD: {vad_icon}, RMS: {rms_icon}({self.smoothed_rms:.1f}/{self.VOICE_THRESHOLD:.1f}), FREQ: {freq_icon}, CNT: {self.consecutive_speech} {speech}", end='\r')
        
        # í†µí•© ê²°ê³¼: VADê°€ trueì´ë©´ì„œ RMSë‚˜ ì£¼íŒŒìˆ˜ ì¤‘ í•˜ë‚˜ë¼ë„ trueì—¬ì•¼ í•¨ (ë” ì—„ê²©í•œ ì¡°ê±´)
        speech_detected = vad_result and (rms_result or freq_result)
        return speech_detected
    
    def check_human_freq(self, audio_data):
        """ì‚¬ëŒ ëª©ì†Œë¦¬ ì£¼íŒŒìˆ˜ ëŒ€ì—­ í™•ì¸"""
        try:
            # ë°ì´í„°ë¥¼ float32ë¡œ ë³€í™˜
            count = len(audio_data) // 2
            format_str = "%dh" % count
            shorts = struct.unpack(format_str, audio_data)
            data = np.array(shorts).astype(np.float32) / 32768.0  # ì •ê·œí™”
            
            # FFT ìˆ˜í–‰
            fft_data = fft(data)
            # ì£¼íŒŒìˆ˜ ì„±ë¶„ ê³„ì‚°
            fft_freqs = np.fft.fftfreq(len(data), 1.0/self.RATE)
            
            # ì‚¬ëŒ ëª©ì†Œë¦¬ ì£¼íŒŒìˆ˜ ëŒ€ì—­ì˜ íŒŒì›Œ ê³„ì‚°
            voice_freq_mask = (fft_freqs >= self.human_freq_low) & (fft_freqs <= self.human_freq_high)
            all_freq_power = np.sum(np.abs(fft_data))
            
            if all_freq_power == 0:
                return False
                
            voice_freq_power = np.sum(np.abs(fft_data[voice_freq_mask]))
            voice_power_ratio = voice_freq_power / all_freq_power
            
            # ì‚¬ëŒ ëª©ì†Œë¦¬ ëŒ€ì—­ì˜ íŒŒì›Œê°€ ì „ì²´ì˜ 10% ì´ìƒì´ë©´ ì‚¬ëŒ ëª©ì†Œë¦¬ë¡œ íŒë‹¨ (20% â†’ 10%ë¡œ ì™„í™”)
            return voice_power_ratio > 0.1
            
        except Exception as e:
            if self.debug_mode:
                print(f"ì£¼íŒŒìˆ˜ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return True  # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’ìœ¼ë¡œ True ë°˜í™˜
    
    def get_rms(self, audio_data):
        """ì˜¤ë””ì˜¤ ë°ì´í„°ì˜ RMS(Root Mean Square) ê°’ ê³„ì‚°"""
        count = len(audio_data) // 2
        format_str = "%dh" % count
        try:
            shorts = struct.unpack(format_str, audio_data)
            shorts_array = np.array(shorts).astype(np.float32)
            sum_squares = np.sum(shorts_array ** 2)
            rms = np.sqrt(max(0, sum_squares / count))
            return rms
        except Exception as e:
            if self.debug_mode:
                print(f"RMS ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0
    
    def start_recording(self):
        """ë…¹ìŒ ì‹œì‘"""
        self.recording = True
        self.frames = []
        self.silence_frames = 0
        print("\nìŒì„± ê°ì§€! ë…¹ìŒ ì‹œì‘..." + " " * 40)
    
    def stop_recording(self):
        """ë…¹ìŒ ì¤‘ì§€ ë° íŒŒì¼ ì €ì¥"""
        if self.recording and len(self.frames) > 0:
            self.recording = False
            print("ë…¹ìŒ ì¤‘ì§€, íŒŒì¼ ì €ì¥ ì¤‘..." + " " * 40)
            
            # í˜„ì¬ ì‹œê°„ì„ íŒŒì¼ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = os.path.join(self.output_dir, f"recording_{timestamp}.wav")
            
            # WAV íŒŒì¼ë¡œ ì €ì¥
            wf = wave.open(filename, 'wb')
            wf.setnchannels(self.CHANNELS)
            wf.setsampwidth(self.p.get_sample_size(self.FORMAT))
            wf.setframerate(self.RATE)
            wf.writeframes(b''.join(self.frames))
            wf.close()
            
            print(f"ë…¹ìŒ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
            print("ë‹¤ì‹œ ìŒì„± ê°ì§€ ëŒ€ê¸° ì¤‘...")
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if hasattr(self, 'stream') and self.stream.is_active():
            self.stream.stop_stream()
            self.stream.close()
        
        self.p.terminate()


if __name__ == "__main__":
    # ë””ë²„ê·¸ ëª¨ë“œë¥¼ í™œì„±í™”í•˜ë ¤ë©´ Trueë¡œ ì„¤ì •
    recorder = ImprovedVoiceRecorder(debug_mode=True)
    recorder.start()